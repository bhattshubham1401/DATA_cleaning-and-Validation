{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "\n",
    "# Assuming 'dframe' is a PySpark DataFrame\n",
    "# If not, you can read your data into a PySpark DataFrame using spark.read.parquet or another appropriate method\n",
    "\n",
    "# Assuming 'id' is a list of location values\n",
    "lst = []\n",
    "for location in id:\n",
    "    # Filter PySpark DataFrame based on location\n",
    "    df = dframe.filter(dframe['sensor'] == location)\n",
    "\n",
    "    # Convert 'Clock' to timestamp and filter based on the date\n",
    "    df = df.withColumn('Clock', F.to_timestamp(df['Clock']))\n",
    "    df = df.filter(df['Clock'] >= '2022-11-18 00:00:00')\n",
    "\n",
    "    # Filter rows with 0 voltage or current\n",
    "    filtered_df = df.filter(\n",
    "        ((df['R_Voltage'] == 0) | (df['Y_Voltage'] == 0) | (df['B_Voltage'] == 0)) &\n",
    "        ((df['R_Current'] == 0) | (df['Y_Current'] == 0) | (df['B_Current'] == 0))\n",
    "    )\n",
    "\n",
    "    # Add 'Kwh' column and set its value to 0 for the filtered rows\n",
    "    filtered_df = filtered_df.withColumn('Kwh', F.lit(0))\n",
    "\n",
    "    # Update original DataFrame with modified rows\n",
    "    df = df.join(filtered_df.select('Clock', 'Kwh'), 'Clock', 'left_outer')\n",
    "    df = df.withColumn('Kwh', F.when(df['Kwh'].isNotNull(), df['Kwh']).otherwise(df['Kwh']))\n",
    "\n",
    "    # Aggregate and resample data\n",
    "    df = df.select('Clock', 'Kwh').groupBy(F.window('Clock', '1 hour')).agg(F.sum('Kwh').alias('Kwh'))\n",
    "    \n",
    "    # Calculate rolling mean\n",
    "    windowSpec = Window.orderBy('window.start').rowsBetween(-23, 0)\n",
    "    df = df.withColumn('Kwh_r', F.avg('Kwh').over(windowSpec))\n",
    "    \n",
    "    # Drop rows with missing values\n",
    "    df = df.na.drop()\n",
    "\n",
    "    lst.append(df)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
